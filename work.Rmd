---
title: "Untitled"
output: 
  html_document: 
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Préparation des couches

## Importer la grille de l'Insee 200m

Les données viennent directement d'ici : [https://www.insee.fr/fr/statistiques/2520034](https://www.insee.fr/fr/statistiques/2520034)

### Téléchargement
```{r, eval=FALSE}
# Téléchargement des données carroyées
download.file(url = "https://www.insee.fr/fr/statistiques/fichier/2520034/200m-carreaux-metropole.zip", 
              destfile = "data/raw/200m-carreaux-metropole.zip")
# extraction du dossier zippé
unzip("data/raw/200m-carreaux-metropole.zip", exdir = "data/raw/")

```


### Import dans R
```{r, eval=FALSE}
library(sf)
# Lecture du fichier mif/mid (en projection Lambert II étendu)
car_m <- st_read("data/raw/200m-carreaux-metropole/car_m.mif", 
                 stringsAsFactors = FALSE, crs =  27572)
# Passage de la projection en LAEA - code EPSG 3035
car_m <- st_transform(car_m, crs = 3035)


```


### Selectionner les carreaux en IDF
Pour des calculs moins lourds ensuite

```{r, eval=FALSE}
# import communes françaises
admin <- st_read('data/raw/LEVEL_INT_PARIS.shp')
# transform vers proj des carreaux
admin <- st_transform(admin, crs = 3035)
# quels carreaux sont en IDF
x <- st_intersects(x = car_m, y = st_union(admin))
car_idf <- car_m[lengths(x) > 0,]

```



### Selection des carreaux rassemblant 5 M d'hab
5M d'hab cumulé autour de l'Hotel de Ville

```{r, eval=FALSE}
library(foreign)
# import de la table de population
pop <- read.dbf('data/raw/200m-carreaux-metropole/car_m.dbf')
# jointure
car_idf <- merge(car_idf, pop[,2:5], by = "idINSPIRE", all.x=TRUE)
# création d'un point pour le centre
centre <- st_sfc(st_point(c(3760756, 2889475)), crs = 3035)
# calclul de la distance au centre
car_idf$dist_centre <- st_distance(x = st_centroid(car_idf), y = centre)
# Selection des carreaux comprenant moins de 5M à partir du centre
car_idf <- car_idf[order(car_idf$dist_centre, decreasing = FALSE), ]
car_idf$cumpop <- cumsum(car_idf$ind_c)
car_idf5M <- car_idf[car_idf$cumpop<=5000000,]
```


## Création de la grille pour l'étude

### Création d'une grille pour l'analyse des tweets
Cette grille a la même emprise globale que la grille de pop, le même diamètre. Elle couvre un espace continu.
```{r, eval=FALSE}
# création de la grille
my_grid <- st_make_grid(x = car_idf5M, cellsize = 200)
my_grid <- st_sf(geometry = my_grid)
# calcul des distance au centre
my_grid$dist_centre <- st_distance(x = st_centroid(my_grid), y = centre)
# Selection des carreaux à une distance < à la distance max de la grille de population 
my_grid <- my_grid[my_grid$dist_centre <= max(car_idf5M$dist_centre),]
```


### Récupérer les données de pop dans la grille continue
Il doit y avoir moyen de faire plus élégant ici...
```{r, eval=FALSE}
# intersection entre les 2 grilles
x <- st_intersects(my_grid, st_centroid(car_idf5M))
xx <- unlist(lapply(x, function(x){if(length(x)>0){x[[1]]}else{NA}}))
my_grid$idINSPIRE <- car_idf5M[xx,c("idINSPIRE"), drop=T]
my_grid$ind_c <- car_idf5M[match(my_grid$idINSPIRE, car_idf5M$idINSPIRE), 
                           "ind_c", drop=T]
```




## Récupérer des couches d'habillage

```{r, eval=FALSE}
library(osmdata)
library(units)
# define a bounding box
q0 <- opq(bbox = st_bbox(st_transform(my_grid, 4326)))

# Récupérer les cours d'eau
q <- add_osm_feature(opq = q0, key = 'waterway', value = "riverbank")
res <- osmdata_sf(q)
river <- do.call(c, list(res$osm_multipolygons$geometry, 
                         res$osm_polygons$geometry))
# nettoyage
river <- st_transform(river, 3035)
river <- st_union(x = st_buffer(river,0), by_feature = F)
river <- st_cast(x = river, 'POLYGON')
river <- river[st_area(river)>=set_units(20000, "m^2")]
river <- st_intersection(x = river, y = my_grid)
river <- st_union(river)

# extract Parks and Cemetaries
q <- add_osm_feature(opq = q0, key = 'leisure', value = "park")
res <- osmdata_sf(q)
parc1 <- st_geometry(res$osm_polygons)
parc2 <- st_geometry(res$osm_multipolygons)
q <- add_osm_feature(opq = q0, key = 'landuse', value = "cemetery")
res <- osmdata_sf(q)
parc3 <- st_geometry(res$osm_polygons)
parc <- do.call(c, list(parc1, parc2, parc3))
# nettoyage
parc <- st_transform(parc, 3035)
parc <- st_union(x = st_buffer(parc,0), by_feature = F)
parc <- st_cast(parc, "POLYGON")
parc <- parc[st_area(parc)>=set_units(30000, "m^2")]
parc <- st_intersection(x = parc, y = my_grid)
parc <- st_union(parc)

```


## Export (temp)

```{r, eval=FALSE}

# j'utilise rds pour les sauvegarde de fichier : 
# 1 objet = 1 fichier, pas de risque d'écraser des objets déjà existants
saveRDS(my_grid, "data/my_grid.rds")
saveRDS(river, "data/river.rds")
saveRDS(parc, "data/parc.rds")

```

## Import des tweets

**On peut démarrer une session à partir d'ici**

```{r, cache=TRUE}
my_grid <- readRDS("data/my_grid.rds")
river <- readRDS("data/river.rds")
parc <- readRDS("data/parc.rds")

```

### Carto

```{r, cache=TRUE}
library(cartography)
opar <- par(mar=c(0,0,0,0))
choroLayer(x = my_grid, var="ind_c", border = NA, nclass = 6, method="q6",
           col=carto.pal(pal1 = 'wine.pal', n1 = 6), colNA = NA, add=F)
plot(river, add=T, col = "#AAD3DF", border="#AAD3DF", lwd= 0.5)
plot(parc, add=T, col = "#AACBAF", border=NA)
par(opar)
```


### Import des tweets et transformation en sf
```{r, eval=FALSE}
library(readr)
# import tweets
tweet_raw <- read_csv("data/raw/tcat_paris_geo-20160101-20161231---------geoTweets--8cef61cd09.csv")
names(tweet_raw)[1] <- "id"
tw <- tweet_raw[, c("id","created_at","from_user_id",  
                    "from_user_name", "text", "lang", "lng", "lat", "source")]
# transform to geo
tw_geo <- st_as_sf(as.data.frame(tw), coords = c("lng","lat"), crs = 4326)
tw_geo <- st_transform(tw_geo, 3035)
nrow(tw_geo)

```

### Selection des tweets dans la grille
et association du code id du carreau
```{r, eval=FALSE}
# dans quel carreau se trouve chaque tweet
inter <- st_intersects(x = tw_geo, y = my_grid, sparse = T)
tw_geo$idINSPIRE <- unlist(lapply(inter, function(x){if(length(x)>0){x[[1]]}else{NA}}))
# selection des tweets dans les carreaux
tw_geo <- tw_geo[!is.na(tw_geo$idINSPIRE),]
```

## Création d'une date/heure correcte

```{r, eval=FALSE}
# Parse la date dans le bon fuseau horaire
library(lubridate)
tw_geo$fulldate <- with_tz(as.POSIXct(tw_geo$created_at), "Europe/Paris")
# dégager les tweets du 1er janvier 2017
tw_geo <- tw_geo[tw_geo$fulldate >= force_tz(as.POSIXct("2016-01-01 00:00:00"), "Europe/Paris" ) &
                      tw_geo$fulldate < force_tz(as.POSIXct("2017-01-01 00:00:00"), "Europe/Paris" ),]

```

## Sélection des tweets en fonctions des clients

```{r, eval=FALSE}
# nb tweets by source / user
tsu <- aggregate(tw_geo$id, 
                 by = list(source = tw_geo$source, 
                           from_user_id = tw_geo$from_user_id), 
                 FUN = length)
# nb user / source
us <- aggregate(tsu$source, 
                by = list(source = tsu$source), 
                FUN = length)
# nb tweet / source
ts <- aggregate(tsu$x, 
                by = list(source = tsu$source), 
                FUN = sum)
# table client/nb tweets/nb users/nb tweet per user/short name
client <- merge(us,ts, by = 'source')
names(client)[2:3] <- c("nuser", "ntweet")
client$tweetpuser <- round(client$ntweet / client$nuser,0)
client <- client[order(client$nuser, decreasing = T),]
row.names(client) <- 1:nrow(client)
client$name <- apply(client,1,function(x){rvest::html_text(xml2::read_html(x[1]))})
saveRDS(client, "data/client.rds")

```


```{r, eval=TRUE, cache=TRUE}
client <- readRDS("data/client.rds")
# graph des clients tweet/user (log log)
options(scipen=1000000)
plot(client$nuser, client$ntweet,
     xlab="n. user", ylab = "n. tweet", 
     main="N user & tweet par client", 
     log="xy")
text(client$nuser[1:4], client$ntweet[1:4], labels = client$name[1:4], 
     cex = 0.7, pos = c(2,4,4,4))

# meme graph avec seulement les clients selectionnés
plot(client[client$nuser >= 10 & client$tweetpuser <= 50, "nuser"], 
     client[client$nuser >= 10 & client$tweetpuser <= 50, "ntweet"], 
     xlab="n. user", ylab = "n. tweet", 
     main="N user & tweet par client après nettoyage clients", 
     log="xy")
text(client$nuser[1:4], client$ntweet[1:4], labels = client$name[1:4], 
     cex = 0.7, pos = c(2,4,4,4))

```

```{r, eval=FALSE}
# Selection des tweets utilisant des clients "normaux"
# c a d pas des robots
tw_geo <- tw_geo[tw_geo$source %in% 
                   client[client$nuser >= 10 & client$tweetpuser <= 50, "source"],]
```


## Ajout des coordonnées en colonnes 
```{r, eval=FALSE}
tw_geo <- cbind(tw_geo, st_coordinates(tw_geo))
```

## Export 
```{r, eval=FALSE}
saveRDS(tw_geo, "data/tw_geo.rds")
```



# Selection avancée des tweets

**On peut démarrer une session à partir d'ici**

```{r, cache=TRUE, eval=TRUE}
library(sf)
tw_geo <- readRDS("data/tw_geo.rds")
```


## Inspection des positions des tweets

<!-- En regardant d'un peu plus près les localisations des tweets, il semble un peu étrange que certains se trouvent à moins d'un mètre de distance (en si grande quantité...) -->


### Création de points de tweets
Les différentes localisations des tweets (aggrégation sur les coordonnées). 
Appelons ça les "points de tweets"

```{r, eval=TRUE, cache=TRUE}
twagcoord <- aggregate(tw_geo$id, 
                       by = list(lng=tw_geo$X, lat=tw_geo$Y), 
                       FUN = length)
out <- twagcoord[order(twagcoord$x, decreasing = T),] 
out$id <- 1:nrow(out)
nrow(out)
nrow(tw_geo)
```

Les 1 040 655 tweets sont émis depuis 164 002 points de tweets

```{r, eval=TRUE, cache=TRUE}
plot(1:nrow(out), out$x, log="y",pch=20, cex=0.4, col="red", ylab = 'nb. tweets', 
     xlab="",main="nb tweets par point de tweets")
```

Les premiers points de tweets concentrent la plupart des tweets.  
`r sum(out$x[1:1000])` tweets (soit `r round(sum(out$x[1:1000])*100/nrow(tw_geo))`%) pour les 1000 premiers points


Ces superpositions de tweets ne doivent être que très rarement issues de position GPS.   
Les tweets se superposant ne sont pas issus de positions GPS mais sont assignés automatiquement à des POI (hotels de villes, batiments touristiques...) ?

MM : oui, étonnant... en regardant, en zoomant sur la carte, on ne voit pas forcément que ces points correspondent à des centroïdes ou entrées de bâtiments, par ex.

```{r, eval=TRUE, cache=TRUE}
# Sélection des 1000 premières paires de coordonnées ; transfo en données géo en WGS84
twagcoord_frequents <- out[1:1000,]
twagcoord_frequents_geo <- st_as_sf(as.data.frame(twagcoord_frequents), coords = c("lng","lat"), crs = 3035)
twagcoord_frequents_geo <- st_transform(twagcoord_frequents_geo, 4326)

# et carto des 100 premiers
library(leaflet)
leaflet() %>% addTiles() %>% 
  addCircleMarkers(data=twagcoord_frequents_geo[1:100,], popup = ~as.character(x),
    radius = 5, color = "red", stroke = FALSE, fillOpacity = 0.5)
```

  
Je m'interroge... (précision à 1 m près ?)

Exemple autour de Bastille = 69ème point (avec 1149 tweets)  
*Sauf qu'il ne veut pas afficher le zoom sur Bastille // pas compris*
```{r, eval=TRUE,cache=TRUE}
leaflet() %>% addTiles() %>% setView(lng=2.369,lat = 48.853,zoom=18) %>% 
  addCircleMarkers(data=twagcoord_frequents_geo[1:100,], popup = ~as.character(x),
                   radius = 5,
                   color = "red",
                   stroke = FALSE, fillOpacity = 1)
```
  
et la carte avec tous les tweets autour de Bastille => l'assignement est curieux ici  
  
![ ](Img/Bastille.png)


### Ces points sont ils stables dans le temps

calendar heat map des 10 premiers points de tweets

```{r, cache=TRUE, fig.height=3}
source("calendarHeat.R")
# selection des tweets du 1er point de tweet

for(i in 1:10){
  xx <- tw_geo[tw_geo$X %in% out[i,"lng"] & tw_geo$Y %in% out[i,"lat"],  ]
  user <- aggregate(xx$id,by=list(u=xx$from_user_id),length)
  my_ts <- aggregate(xx$id,by = list(date=substr(xx$fulldate,1,10)), length)
  cat(c("i:",i, "; tweets:",sum(my_ts$x),"; user:", nrow(user)))
  calendarHeat(my_ts$date, my_ts$x,ncolor=99, col = cartography::carto.pal(pal1 =  "blue.pal",20))
}

```

Et bien non, ces points ne sont pas stables dans le temps. 




### Ces points sont ils précisément placés

Je m'interesse ici aux points de tweets placés à moins d'un mètre les uns des autres. 

```{r, eval=TRUE, cache=TRUE}
# transformation des 1000 premiers points de tweets en objet sf
x <- st_as_sf(as.data.frame(out[1:1000,]), coords = c("lng","lat"), crs = 3035)
# distance entre points de tweets 
mat <- st_distance(x,x)
# se débarasser des unités 
units(mat) <- NULL
# neutralisation de la diagonale
diag(mat) <- 9999
# mat contient les distances entre tous les points de tweets
```




```{r, eval=TRUE, cache=TRUE}
# a donne la liste des points de tweets ayant des points de tweets à moins de 1 mètre
a <- apply(mat,2,min) < 1
# les indices de ces points
i <- which(a)[1]
j <- which(mat[i,]<1)

# i et j nous donnent une première liste de 4 points placés à moins d'un mêtre 
# les uns des autres
c(i,j)

# selection des tweets 
xx <- tw_geo[tw_geo$X %in% out[c(i,j),"lng"] & tw_geo$Y %in% out[c(i,j),"lat"],  ]
user <- aggregate(xx$id, by = list(u = xx$from_user_id),length)
my_ts <- aggregate(xx$id, by = list(date = substr(xx$fulldate,1,10)), length)
cat(c("tweets:",sum(my_ts$x),"; user:", nrow(user)))
calendarHeat(my_ts$date, my_ts$x,ncolor=99, col = cartography::carto.pal(pal1 =  "blue.pal",20))
```

Ces points ne sont pas présents tout au long de l'année.  
Mais il ya mieux :


```{r, eval=TRUE, cache=TRUE}
for(k in c(i,j)){
  xx <- tw_geo[tw_geo$X %in% out[k,"lng"] & tw_geo$Y %in% out[k,"lat"],  ]
  user <- aggregate(xx$id,by=list(u=xx$from_user_id),length)
  my_ts <- aggregate(xx$id,by = list(date=substr(xx$fulldate,1,10)), length)
  cat(c("i:",k, "; tweets:",sum(my_ts$x),"; user:", nrow(user)))
  calendarHeat(my_ts$date, my_ts$x,ncolor=99, col = cartography::carto.pal(pal1 =  "blue.pal",20))
}

```


Ces points ne sont pas présents simultanément au cours de l'année.   


Faut-il ne considérer que les tweets ne se supperposant pas? Ou alors ce n'est pas grave que les tweets se superposent, 
1000 points sur le même metre carré à montmartre doivent bien représenter 1000 personnes dans les environs (quelques metres à la ronde, quelque centaines de metres?). Quid des 145000 tweets sur la place de l'hotel de ville? 


  
MM : déprimant... à ne plus rien comprendre


```{r, eval=FALSE, echo=FALSE}
library(sf)
out [1,]
my_grid <- readRDS("data/my_grid.rds")
river <- readRDS("data/river.rds")
parc <- readRDS("data/parc.rds")
a <- out[out$x==1,]
xx <- tw_geo[tw_geo$X %in% a[,"lng"] & tw_geo$Y %in% a[,"lat"],  ]
library(spatstat)
library(maptools)
library(raster)
bb <- as(my_grid, "Spatial")
bbowin <- as.owin(bb)
pts <- st_coordinates(xx)
p <- ppp(pts[,1], pts[,2], window=bbowin)
ds <- density.ppp(x = p, sigma = 300, eps = c(100,100))
rasdens <- raster(ds) * 1000 * 1000
rasdens <- rasdens+1
bks <- getBreaks(values(rasdens), nclass = 20, method = "equal")
cols <- colorRampPalette(c("black","#940000", "yellow","white"), interpolate = "spline")(length(bks)-1)
plot(my_grid$geometry, col = NA, border = NA, main="",bg="black")
image(rasdens, breaks= bks, col=cols, add = T,legend=F)
legendChoro(pos = "topright",cex = 0.7,title.cex = .8,
            title.txt = paste0("\nsigma=500m,\nn=",nrow(pts)),
            breaks = bks-1, nodata = FALSE,values.rnd = 0,
            col = cols)
plot(river, col = "#AAD3DF", add=T, border=NA)
plot(parc, col = "#CDEBB235", border = NA, add=T)
plot(st_geometry(xx), add=T, col = "#7F78D010", pch = 20, cex = 0.3)

plot(st_geometry(xx), add=T, col = "#7F78D010", pch = 20, cex = 0.2)

plot(st_geometry(xx), add=T, col = "#7F78D010", pch = 20, cex = 0.1)
barscale(size = 1)
mtext(text = "cartography 2.0.2\nMap data © OpenStreetMap contributors, under CC BY SA.", 
      side = 1, line = -1, adj = c(0.01,0.01), cex = 0.6, font = 3)
north(pos = c(661171.8,6858051))
par(opar)

```






